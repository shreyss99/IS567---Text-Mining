{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries here that you need for different processing steps\n",
    "import nltk\n",
    "import csv\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT REPRESENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-to-vector\n",
    "\n",
    "You can implement any or many of these approaches to try out which one produces the best results ultimately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read a dataframe for understanding text-representation\n",
    "\n",
    "data_df = pd.read_csv(\"Dataset/covid.csv\")\n",
    "\n",
    "print (\"Data set: \", len(data_df))\n",
    "\n",
    "display(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explaining one-hot-encoding using one of the instances from the dataset. \n",
    "# Remember we talked about the sparsity in this approach and how this could be bad design for very large datasets.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "docs = data_df.iloc[6][\"OriginalTweet\"].split()\n",
    "print(len(docs),docs,\"\\n\")\n",
    "\n",
    "# split documents to tokens\n",
    "tokens_docs = [doc.split(\" \") for doc in docs]\n",
    "\n",
    "# convert list of of token-lists to one flat list of tokens\n",
    "# and then create a dictionary that maps word to id of word,\n",
    "all_tokens = itertools.chain.from_iterable(tokens_docs)\n",
    "word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}\n",
    "print(len(word_to_id),word_to_id,\"\\n\")\n",
    "\n",
    "# convert token lists to token-id lists\n",
    "token_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]\n",
    "\n",
    "# convert list of token-id lists to one-hot representation\n",
    "vec = OneHotEncoder(categories=\"auto\")\n",
    "X = vec.fit_transform(token_ids)\n",
    "X = X.toarray()\n",
    "X\n",
    "# print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words BOW- CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As discussed about the CountVectorizer, below is implemented for only a set of instances for exploration \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# text = [\"i love nlp. nlp is so cool\"]\n",
    "text = data_df.iloc[1:4][\"OriginalTweet\"]\n",
    "print(text,\"\\n\")\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "print(vectorizer.vocabulary_,\"\\n\")\n",
    "# encode document\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "\n",
    "print(vector.shape,\"\\n\") \n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency- Inverse Document Frequency (TF-IDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As discussed about the TF-IDF, below is implemented for only a set of instances for exploration \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# text1 = ['i love nlp', 'nlp is so cool', \n",
    "# 'nlp is all about helping machines process language', \n",
    "# 'this tutorial is on basic nlp technique']\n",
    "# print(text1)\n",
    "text1 = data_df.iloc[1:6][\"OriginalTweet\"].tolist()\n",
    "print(text1)\n",
    "tf = TfidfVectorizer()\n",
    "txt_fitted = tf.fit(text1)\n",
    "txt_transformed = txt_fitted.transform(text1)\n",
    "\n",
    "idf = tf.idf_\n",
    "print(dict(zip(txt_fitted.get_feature_names_out(), idf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UniGrams, BiGrams, TriGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "def ngram_convertor(sentence,n=3):\n",
    "    ngram_sentence = ngrams(sentence.split(), n)\n",
    "    for item in ngram_sentence:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this function with different values of sentence and N to see the grams building from the sentence.\n",
    "\n",
    "sentence = data_df.iloc[2][\"OriginalTweet\"]\n",
    "print(sentence)\n",
    "N = 3\n",
    "ngram_convertor(sentence,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
